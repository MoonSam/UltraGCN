Amazon_Line_Prof
###################### UltraGCN ######################
1. Loading Configuration...
Filename: main.py

Line #    Mem usage    Increment  Occurences   Line Contents
============================================================
   151    198.0 MiB    198.0 MiB           1   @profile
   152                                         def load_data(train_file, test_file):
   153    198.0 MiB      0.0 MiB           1       trainUniqueUsers, trainItem, trainUser = [], [], []
   154    198.0 MiB      0.0 MiB           1       testUniqueUsers, testItem, testUser = [], [], []
   155    198.0 MiB      0.0 MiB           1       n_user, m_item = 0, 0
   156    198.0 MiB      0.0 MiB           1       trainDataSize, testDataSize = 0, 0
   157    198.0 MiB      0.0 MiB           1       with open(train_file, 'r') as f:
   158    328.1 MiB     -9.2 MiB       52644           for l in f.readlines():
   159    328.1 MiB      0.0 MiB       52643               if len(l) > 0:
   160    328.1 MiB     20.6 MiB       52643                   l = l.strip('\n').split(' ')
   161    328.1 MiB    -91.4 MiB     2538659                   items = [int(i) for i in l[1:]]
   162    328.1 MiB     -0.8 MiB       52643                   uid = int(l[0])
   163    328.1 MiB     -0.4 MiB       52643                   trainUniqueUsers.append(uid)
   164    328.1 MiB     13.9 MiB       52643                   trainUser.extend([uid] * len(items))
   165    328.1 MiB     15.6 MiB       52643                   trainItem.extend(items)
   166    328.1 MiB    -16.1 MiB       52643                   m_item = max(m_item, max(items))
   167    328.1 MiB    -16.3 MiB       52643                   n_user = max(n_user, uid)
   168    328.1 MiB    -16.3 MiB       52643                   trainDataSize += len(items)
   169    318.0 MiB    -10.2 MiB           1       trainUniqueUsers = np.array(trainUniqueUsers)
   170    318.0 MiB      0.0 MiB           1       trainUser = np.array(trainUser)
   171    243.1 MiB    -74.9 MiB           1       trainItem = np.array(trainItem)
   172                                         
   173    243.1 MiB      0.0 MiB           1       with open(test_file) as f:
   174    275.6 MiB      0.2 MiB       52644           for l in f.readlines():
   175    275.6 MiB      0.0 MiB       52643               if len(l) > 0:
   176    275.6 MiB      7.9 MiB       52643                   l = l.strip('\n').split(' ')
   177    275.6 MiB      0.0 MiB       52643                   try:
   178    275.6 MiB     10.8 MiB      761307                       items = [int(i) for i in l[1:]]
   179    275.6 MiB      0.0 MiB           4                   except:
   180    275.6 MiB      0.0 MiB           4                       items = []
   181    275.6 MiB      0.0 MiB       52643                   uid = int(l[0])
   182    275.6 MiB      1.7 MiB       52643                   testUniqueUsers.append(uid)
   183    275.6 MiB      2.1 MiB       52643                   testUser.extend([uid] * len(items))
   184    275.6 MiB      5.2 MiB       52643                   testItem.extend(items)
   185    275.6 MiB      0.0 MiB       52643                   try:
   186    275.6 MiB      0.0 MiB       52643                       m_item = max(m_item, max(items))
   187    275.6 MiB      0.0 MiB           4                   except:
   188    275.6 MiB      0.0 MiB           4                       m_item = m_item
   189    275.6 MiB      0.0 MiB       52643                   n_user = max(n_user, uid)
   190    275.6 MiB      0.0 MiB       52643                   testDataSize += len(items)
   191                                         
   192                                         
   193    271.0 MiB     -4.6 MiB           1       train_data = []
   194    271.0 MiB      0.0 MiB           1       test_data = []
   195                                         
   196    271.0 MiB      0.0 MiB           1       n_user += 1
   197    271.0 MiB      0.0 MiB           1       m_item += 1
   198                                         
   199    657.8 MiB    146.6 MiB     2380731       for i in range(len(trainUser)):
   200    657.8 MiB    240.2 MiB     2380730           train_data.append([trainUser[i], trainItem[i]])
   201    718.9 MiB      0.0 MiB      603379       for i in range(len(testUser)):
   202    718.9 MiB     61.1 MiB      603378           test_data.append([testUser[i], testItem[i]])
   203    718.9 MiB      0.0 MiB           1       train_mat = sp.dok_matrix((n_user, m_item), dtype=np.float32)
   204                                         
   205   1221.6 MiB    225.9 MiB     2380731       for x in train_data:
   206   1221.6 MiB    276.8 MiB     2380730           train_mat[x[0], x[1]] = 1.0
   207                                         
   208                                         
   209                                             # construct degree matrix for graphmf
   210                                         
   211   1242.4 MiB     20.7 MiB           1       items_D = np.sum(train_mat, axis = 0).reshape(-1)
   212   1242.4 MiB      0.0 MiB           1       users_D = np.sum(train_mat, axis = 1).reshape(-1)
   213                                         
   214   1242.4 MiB      0.0 MiB           1       beta_uD = (np.sqrt(users_D + 1) / users_D).reshape(-1, 1)
   215   1242.4 MiB      0.0 MiB           1       beta_iD = (1 / np.sqrt(items_D + 1)).reshape(1, -1)
   216                                         
   217   1242.4 MiB      0.0 MiB           1       constraint_mat = {"beta_uD": torch.from_numpy(beta_uD).reshape(-1),
   218   1242.4 MiB      0.0 MiB           1                         "beta_iD": torch.from_numpy(beta_iD).reshape(-1)}
   219                                         
   220   1242.4 MiB      0.0 MiB           1       return train_data, test_data, train_mat, n_user, m_item, constraint_mat


Computing \Omega for the item-item graph... 
i-i constraint matrix 0 ok
i-i constraint matrix 15000 ok
i-i constraint matrix 30000 ok
i-i constraint matrix 45000 ok
i-i constraint matrix 60000 ok
i-i constraint matrix 75000 ok
i-i constraint matrix 90000 ok
Computation \Omega OK!
store object in path = ./amazon_ii_neighbor_mat ok
store object in path = ./amazon_ii_constraint_mat ok
Load Configuration OK, show them below
Configuration:
{'embedding_dim': 64, 'ii_neighbor_num': 10, 'model_save_path': './ultragcn_amazon.pt', 'max_epoch': 10, 'enable_tensorboard': True, 'initial_weight': 0.0001, 'dataset': 'amazon', 'gpu': '0', 'device': device(type='cuda', index=0), 'lr': 0.001, 'batch_size': 512, 'early_stop_epoch': 15, 'w1': 1e-08, 'w2': 1.0, 'w3': 1.0, 'w4': 1e-08, 'negative_num': 500, 'negative_weight': 500.0, 'gamma': 0.0001, 'lambda': 2.75, 'sampling_sift_pos': False, 'test_batch_size': 2048, 'topk': 20, 'user_num': 52643, 'item_num': 91599}
Total training batches = 4650
The time for epoch 0 is: train time = 00: 01: 37, test time = 00: 01: 46
Loss = 853.93469, F1-score: 0.002379 	 Precision: 0.00176	 Recall: 0.00366	NDCG: 0.00286
The time for epoch 5 is: train time = 00: 01: 36, test time = 00: 00: 15
Loss = 431.67148, F1-score: 0.011015 	 Precision: 0.00807	 Recall: 0.01735	NDCG: 0.01415
Training end!
END
Wrote profile results to main.py.lprof
